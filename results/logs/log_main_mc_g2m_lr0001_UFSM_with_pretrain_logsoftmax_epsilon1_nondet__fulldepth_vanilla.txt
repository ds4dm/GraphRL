nohup: ignoring input
[[1 2 3]
 [4 5 6]]
| Xavier Initialization
| Xavier Initialization
Use Critic:
False
/local_workspace/liudefen/PycharmProjects/PycharmProjects/PycharmProjects/gnn/GraphRL/gcn/models_gcn.py:85: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  features = F.log_softmax(features.t())
epochs 0 loss -465.3736877441406
epoch 0000 gcn2min_degree  train av_ratio 0.918450487796162  validation av_ratio 0.959252104012613
parameter name actor.gc1.weight parameter value tensor(1.00000e-04 *
       [[-7.0304]], device='cuda:0')
parameter name actor.gc1.bias parameter value tensor([ 1.3975], device='cuda:0')
parameter name actor.gc2.weight parameter value tensor([[-1.2467]], device='cuda:0')
parameter name actor.gc2.bias parameter value tensor(1.00000e-04 *
       [ 2.2029], device='cuda:0')
epochs 1 loss -227.22735595703125
epochs 2 loss -419.8795166015625
epochs 3 loss -381.72457122802734
epochs 4 loss -524.7979888916016
epochs 5 loss -568.6089096069336
epochs 6 loss -335.8544616699219
epochs 7 loss -588.9829788208008
epochs 8 loss -332.22483825683594
epochs 9 loss -576.3117370605469
epochs 10 loss -473.1024932861328
epochs 11 loss -404.7659606933594
epochs 12 loss -290.5031280517578
epochs 13 loss -514.0108108520508
epochs 14 loss -442.64390563964844
epochs 15 loss -665.1593627929688
epochs 16 loss -564.3755645751953
epochs 17 loss -564.9389190673828
epochs 18 loss -395.0081481933594
epochs 19 loss -495.8714294433594
epochs 20 loss -243.526123046875
epochs 21 loss -357.5250244140625
epochs 22 loss -331.6625213623047
epochs 23 loss -469.2306671142578
epochs 24 loss -441.24607849121094
epochs 25 loss -287.24070739746094
epochs 26 loss -266.9297561645508
epochs 27 loss -346.8587188720703
epochs 28 loss -572.0116500854492
epochs 29 loss -517.6713104248047
epochs 30 loss -373.09971618652344
epochs 31 loss -685.1607551574707
epochs 32 loss -698.7578887939453
epochs 33 loss -394.72462463378906
epochs 34 loss -558.4593200683594
epochs 35 loss -591.3809585571289
epochs 36 loss -447.60662841796875
epochs 37 loss -568.1306838989258
epochs 38 loss -320.683349609375
epochs 39 loss -580.5634536743164
epochs 40 loss -589.2006072998047
epochs 41 loss -589.0485458374023
epochs 42 loss -476.0585479736328
epochs 43 loss -394.8392028808594
epochs 44 loss -395.78387451171875
epochs 45 loss -388.3583526611328
epochs 46 loss -462.40584564208984
epochs 47 loss -432.8162384033203
epochs 48 loss -410.17457580566406
epochs 49 loss -283.35459899902344
epochs 50 loss -334.13368225097656
epochs 51 loss -478.7861328125
epochs 52 loss -485.8585968017578
epochs 53 loss -504.66382598876953
epochs 54 loss -595.339599609375
epochs 55 loss -378.17115783691406
epochs 56 loss -406.0266571044922
epochs 57 loss -433.05787658691406
epochs 58 loss -340.2316436767578
epochs 59 loss -380.95372009277344
epochs 60 loss -404.4728546142578
epochs 61 loss -431.1647491455078
epochs 62 loss -529.2992706298828
epochs 63 loss -396.1947326660156
epochs 64 loss -435.788818359375
epochs 65 loss -450.5557556152344
epochs 66 loss -439.1177978515625
epochs 67 loss -470.61761474609375
epochs 68 loss -513.0463562011719
epochs 69 loss -506.12646484375
epochs 70 loss -350.03273010253906
epochs 71 loss -383.0613250732422
epochs 72 loss -422.8652114868164
epochs 73 loss -353.9748077392578
epochs 74 loss -597.5228576660156
epochs 75 loss -348.56068420410156
epochs 76 loss -300.31683349609375
epochs 77 loss -541.3602905273438
epochs 78 loss -506.4320068359375
epochs 79 loss -313.4795379638672
epochs 80 loss -458.53794860839844
epochs 81 loss -468.08221435546875
