nohup: ignoring input
[[1 2 3]
 [4 5 6]]
| Xavier Initialization
| Xavier Initialization
Use Critic:
False
/local_workspace/liudefen/PycharmProjects/PycharmProjects/GraphRL/gcn/models_gcn.py:85: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  features = F.log_softmax(features.t())
epochs 0 loss 7454.4368896484375
parameter name actor.gc1.weight parameter value tensor(1.00000e-04 *
       [[-7.0304]], device='cuda:0')
parameter name actor.gc1.bias parameter value tensor([ 1.3975], device='cuda:0')
parameter name actor.gc2.weight parameter value tensor([[-1.2467]], device='cuda:0')
parameter name actor.gc2.bias parameter value tensor(1.00000e-04 *
       [ 2.2029], device='cuda:0')
epochs 2 loss 7312.6064453125
epochs 3 loss 6984.2591552734375
epochs 4 loss 6975.166015625
epochs 5 loss 6639.793212890625
epochs 6 loss 6303.0882568359375
epochs 7 loss 6230.61669921875
epochs 8 loss 6373.9718017578125
epochs 9 loss 5770.63037109375
epochs 10 loss 6130.449951171875
epochs 11 loss 5815.5570068359375
epochs 12 loss 5764.713623046875
epochs 13 loss 5928.784729003906
epochs 14 loss 5950.79345703125
epochs 15 loss 6071.8680419921875
epochs 16 loss 6767.98388671875
epochs 17 loss 6735.4029541015625
epochs 18 loss 6979.8013916015625
epochs 19 loss 6996.7686767578125
epochs 20 loss 6837.6998291015625
epochs 21 loss 7052.057373046875
epochs 22 loss 7016.244384765625
epochs 23 loss 7334.9371337890625
epochs 24 loss 7437.9638671875
epochs 25 loss 7082.5037841796875
epochs 26 loss 7025.3948974609375
epochs 27 loss 7130.1314697265625
epochs 28 loss 7114.3536376953125
epochs 29 loss 7058.935791015625
epochs 30 loss 7015.8148193359375
epochs 31 loss 6997.428955078125
epochs 32 loss 7156.0406494140625
epochs 33 loss 7289.0731201171875
epochs 34 loss 6955.1495361328125
epochs 35 loss 7075.778076171875
epochs 36 loss 7225.4500732421875
epochs 37 loss 7049.1334228515625
epochs 38 loss 7150.0101318359375
epochs 39 loss 7098.0325927734375
epochs 40 loss 6503.6763916015625
epochs 41 loss 6525.8973388671875
epochs 42 loss 6342.08935546875
epochs 43 loss 6331.2882080078125
epochs 44 loss 5916.595031738281
epochs 45 loss 5851.3060302734375
epochs 46 loss 5832.558776855469
epochs 47 loss 6615.9415283203125
epochs 48 loss 5770.109069824219
epochs 49 loss 5622.7244873046875
epochs 50 loss 5950.381530761719
epochs 51 loss 5765.860168457031
epochs 52 loss 5868.122406005859
epochs 53 loss 5798.207733154297
epochs 54 loss 5388.897399902344
epochs 55 loss 5751.35595703125
epochs 56 loss 5810.4801025390625
epochs 57 loss 5488.2403564453125
epochs 58 loss 5891.786926269531
epochs 59 loss 6071.592590332031
epochs 60 loss 5909.880859375
epochs 61 loss 5650.815246582031
epochs 62 loss 5745.2938232421875
epochs 63 loss 5907.18212890625
epochs 64 loss 6173.302978515625
epochs 65 loss 5843.2060546875
epochs 66 loss 5674.9451904296875
epochs 67 loss 5918.923889160156
epochs 68 loss 6501.411682128906
epochs 69 loss 6369.582092285156
epochs 70 loss 6071.1734619140625
epochs 71 loss 5918.1549072265625
epochs 72 loss 6030.7413330078125
epochs 73 loss 6137.2176513671875
epochs 74 loss 5542.7994384765625
epochs 75 loss 6045.447570800781
epochs 76 loss 5858.18359375
epochs 77 loss 6123.9627685546875
epochs 78 loss 5984.2359619140625
epochs 79 loss 5967.7552490234375
epochs 80 loss 5959.286193847656
epochs 81 loss 6225.6502685546875
epochs 82 loss 6010.656494140625
epochs 83 loss 5945.4456787109375
epochs 84 loss 5593.7216796875
epochs 85 loss 6028.706604003906
epochs 86 loss 5786.957672119141
epochs 87 loss 5692.042724609375
epochs 88 loss 5104.012359619141
epochs 89 loss 5163.242950439453
epochs 90 loss 5515.869873046875
epochs 91 loss 4895.488006591797
epochs 92 loss 5219.016540527344
epochs 93 loss 5711.625274658203
epochs 94 loss 5258.392059326172
epochs 95 loss 5327.640319824219
epochs 96 loss 5641.130310058594
epochs 97 loss 5691.255798339844
epochs 98 loss 5535.585235595703
epochs 99 loss 5668.2015380859375
epochs 100 loss 5914.9254150390625
epochs 101 loss 5695.296569824219
epochs 102 loss 5324.225769042969
epochs 103 loss 5072.562103271484
epochs 104 loss 5076.586090087891
epochs 105 loss 5139.2083740234375
epochs 106 loss 5229.460266113281
epochs 107 loss 5543.934967041016
epochs 108 loss 5946.80419921875
epochs 109 loss 6328.0386962890625
epochs 110 loss 5570.780517578125
epochs 111 loss 5924.226013183594
epochs 112 loss 5874.8765869140625
epochs 113 loss 5834.858581542969
epochs 114 loss 6268.3402099609375
epochs 115 loss 5942.440185546875
epochs 116 loss 5996.562805175781
epochs 117 loss 5991.770263671875
epochs 118 loss 6514.227783203125
epochs 119 loss 6432.4061279296875
epochs 120 loss 6584.8114013671875
epochs 121 loss 6927.9205322265625
epochs 122 loss 7012.916259765625
epochs 123 loss 6738.389892578125
epochs 124 loss 6969.3131103515625
epochs 125 loss 6593.1219482421875
epochs 126 loss 6882.5421142578125
epochs 127 loss 6895.875732421875
epochs 128 loss 7053.683349609375
epochs 129 loss 7094.305419921875
epochs 130 loss 7211.8175048828125
epochs 131 loss 7290.16455078125
epochs 132 loss 7152.7410888671875
epochs 133 loss 7277.8209228515625
epochs 134 loss 7297.9569091796875
epochs 135 loss 7051.14697265625
epochs 136 loss 7235.3878173828125
epochs 137 loss 7172.53369140625
epochs 138 loss 7308.9881591796875
epochs 139 loss 7068.7755126953125
epochs 140 loss 7021.6910400390625
epochs 141 loss 7121.2203369140625
epochs 142 loss 7130.3594970703125
epochs 143 loss 7289.888671875
epochs 144 loss 6816.680419921875
epochs 145 loss 6813.242431640625
epochs 146 loss 7298.1820068359375
epochs 147 loss 7491.264404296875
epochs 148 loss 7200.7764892578125
epochs 149 loss 7122.722900390625
epochs 150 loss 6870.2664794921875
epochs 151 loss 7009.79541015625
epochs 152 loss 6904.0211181640625
epochs 153 loss 6667.3026123046875
epochs 154 loss 6398.15478515625
epochs 155 loss 6466.1114501953125
epochs 156 loss 6861.0440673828125
epochs 157 loss 7029.956787109375
epochs 158 loss 7092.8887939453125
epochs 159 loss 7132.275390625
epochs 160 loss 7172.7490234375
epochs 161 loss 7097.086669921875
epochs 162 loss 7147.30908203125
epochs 163 loss 6996.585205078125
epochs 164 loss 6991.5887451171875
epochs 165 loss 6792.2679443359375
epochs 166 loss 6899.741455078125
epochs 167 loss 6620.5390625
epochs 168 loss 6715.170166015625
epochs 169 loss 7047.815673828125
epochs 170 loss 7109.56201171875
epochs 171 loss 7261.623046875
epochs 172 loss 6979.8922119140625
epochs 173 loss 7163.6934814453125
epochs 174 loss 7255.3011474609375
epochs 175 loss 7147.04736328125
epochs 176 loss 7275.858642578125
epochs 177 loss 7348.4071044921875
epochs 178 loss 6946.7462158203125
epochs 179 loss 7080.4990234375
epochs 180 loss 7024.26806640625
epochs 181 loss 7057.58154296875
epochs 182 loss 7385.4365234375
epochs 183 loss 7156.7691650390625
epochs 184 loss 7051.1717529296875
epochs 185 loss 7133.0772705078125
epochs 186 loss 7338.5570068359375
epochs 187 loss 7035.907958984375
epochs 188 loss 7178.2606201171875
epochs 189 loss 7300.5726318359375
epochs 190 loss 7345.6951904296875
epochs 191 loss 7186.4619140625
epochs 192 loss 7254.1907958984375
epochs 193 loss 7256.845947265625
epochs 194 loss 7063.8297119140625
epochs 195 loss 7225.23779296875
epochs 196 loss 7292.9366455078125
epochs 197 loss 7115.0330810546875
epochs 198 loss 7322.42138671875
epochs 199 loss 7111.0885009765625
epochs 200 loss 7264.7720947265625
epochs 201 loss 7284.122314453125
epochs 202 loss 7015.428955078125
epochs 203 loss 7257.609130859375
epochs 204 loss 7380.2371826171875
epochs 205 loss 7151.8760986328125
epochs 206 loss 7132.8687744140625
epochs 207 loss 7256.02294921875
epochs 208 loss 7327.8153076171875
epochs 209 loss 7031.7723388671875
epochs 210 loss 7433.5577392578125
epochs 211 loss 7289.2540283203125
epochs 212 loss 7191.7462158203125
epochs 213 loss 7156.93798828125
